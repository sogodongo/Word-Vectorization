{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorization - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll tokenize and vectorize text documents, create and use a bag of words, and identify words unique to individual documents using TF-IDF vectorization. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab we will:  \n",
    "\n",
    "- Implement tokenization and count vectorization from scratch \n",
    "- Implement TF-IDF from scratch \n",
    "- Use dimensionality reduction on vectorized text data to create and interpret visualizations \n",
    "\n",
    "## Let's get started!\n",
    "\n",
    "Run the cell below to import everything necessary for this lab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Corpus\n",
    "\n",
    "In this lab, we'll be working with 20 different documents, each containing song lyrics from either Garth Brooks or Kendrick Lamar albums.  \n",
    "\n",
    "The songs are contained within the `data` subdirectory, contained within the same folder as this lab.  Each song is stored in a single file, with files ranging from `song1.txt` to `song20.txt`.  \n",
    "\n",
    "To make it easy to read in all of the documents, use a list comprehension to create a list containing the name of every single song file in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lyrics_url.txt',\n",
       " 'song1.txt',\n",
       " 'song10.txt',\n",
       " 'song11.txt',\n",
       " 'song12.txt',\n",
       " 'song13.txt',\n",
       " 'song14.txt',\n",
       " 'song15.txt',\n",
       " 'song16.txt',\n",
       " 'song17.txt',\n",
       " 'song18.txt',\n",
       " 'song19.txt',\n",
       " 'song2.txt',\n",
       " 'song20.txt',\n",
       " 'song3.txt',\n",
       " 'song4.txt',\n",
       " 'song5.txt',\n",
       " 'song6.txt',\n",
       " 'song7.txt',\n",
       " 'song8.txt',\n",
       " 'song9.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory path where the song files are located\n",
    "directory_path = \"data\"\n",
    "\n",
    "# List comprehension to get all the file names in the directory\n",
    "filenames = [filename for filename in os.listdir(directory_path) if filename.endswith(\".txt\")]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import a single song to see what our text looks like so that we can make sure we clean and tokenize it correctly. \n",
    "\n",
    "Use the code in the cell below to read in the lyrics from `song18.txt` as a list of lines, just using vanilla Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Kendrick Lamar:]\\n',\n",
       " \"Two wrongs don't make us right away\\n\",\n",
       " \"Tell me something's wrong\\n\",\n",
       " 'Party all of our lives away\\n',\n",
       " 'To take you on\\n',\n",
       " '[Zacari:]\\n',\n",
       " 'Oh, baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'All night (all night, all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " 'All night (all night, all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " '[Babes Wodumo:]\\n',\n",
       " 'Oh my word oh my gosh oh my word (Oh my gosh)\\n',\n",
       " 'Oh my word oh my gosh oh my word (Oh my gosh)\\n',\n",
       " 'Oh my word oh my gosh oh my word (Oh my gosh)\\n',\n",
       " 'Oh my word oh my gosh oh my word (Oh my gosh)\\n',\n",
       " 'Everybody say kikiritikiki (kikiritikiki)\\n',\n",
       " 'Everybody say kikiritikiki (kikiritikiki)\\n',\n",
       " 'Everybody say kikiritikiki (kikiritikiki)\\n',\n",
       " 'Everybody say kikiritikiki (kikiritikiki)\\n',\n",
       " \"Ung'bambe, ung'dedele. Ung'bhasobhe, ung'gudluke\\n\",\n",
       " \"Ung'bambe, ung'dedele. Ung'bhasobhe, ung'gudluke\\n\",\n",
       " \"Ung'bambe, ung'dedele. Ung'bhasobhe, ung'gudluke\\n\",\n",
       " \"Ung'bambe, ung'dedele. Ung'bhasobhe, ung'gudluke\\n\",\n",
       " '[Zacari:]\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'All night (all night all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " 'All night (all night all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " '[Kendrick Lamar:]\\n',\n",
       " '(We go)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " '[?]\\n',\n",
       " '[Zacari:]\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'All night (all night all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " 'All night (all night all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and print song18.txt\n",
    "with open('data/song18.txt') as f:\n",
    "    test_song = f.readlines()\n",
    "    \n",
    "test_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing our Data\n",
    "\n",
    "Before we can create a bag of words or vectorize each document, we need to clean it up and split each song into an array of individual words.\n",
    "\n",
    "Consider the following sentences from the example above:\n",
    "\n",
    "`\"Two wrongs don't make us right away\\n\", \"Tell me something's wrong\\n\"`\n",
    "\n",
    "After tokenization, this should look like:\n",
    "\n",
    "`['two', 'wrongs', 'dont', 'make', 'us', 'right', 'away', 'tell', 'me', 'somethings', 'wrong']`\n",
    "\n",
    "Tokenization is pretty tedious if we handle it manually, and would probably make use of regular expressions, which is outside the scope of this lab. In order to keep this lab moving, we'll use a library function to clean and tokenize our data so that we can move onto vectorization.  \n",
    "\n",
    "Tokenization is a required task for just about any Natural Language Processing (NLP) task, so great industry-standard tools exist to tokenize things for us, so that we can spend our time on more important tasks without getting bogged down hunting every special symbol or punctuation in a massive dataset. For this lab, we'll make use of the tokenizer in the amazing `nltk` library, which is short for _Natural Language Tool Kit_.\n",
    "\n",
    "**_NOTE:_** NLTK requires extra installation methods to be run the first time certain methods are used.  If `nltk` throws you an error about needing to install additional packages, follow the instructions in the error message to install the dependencies, and then rerun the cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, however, we need to do one more step! Computers are very particular about strings. If we tokenized our data in its current state, we would run into the following problems:\n",
    "\n",
    "- Counting things that aren't actually words.  In the example above, `\"[Kendrick Lamar:]\"` is a note specifying who is speaking, not a lyric contained in the actual song, so it should be removed. \n",
    "- Punctuation and capitalization would mess up our word counts. To the Python interpreter, `all`, `All`, and `(all` are unique words, and would all be counted separately.  We need to remove punctuation and capitalization, so that all words will be counted correctly. \n",
    "\n",
    "Before we tokenize our songs, we'll do only a small manual bit of cleaning. \n",
    "\n",
    "In the cell below, write a function to:\n",
    "- remove lines that just contain `['artist names']`\n",
    "- join the list of strings into one big string for the entire song\n",
    "- remove newline characters `\\n`\n",
    "- remove the following punctuation marks: `\",.'?!()\"`\n",
    "- make every word lowercase\n",
    "\n",
    "Test the function on `test_song` to show that it has successfully removed `'[Kendrick Lamar:]'` and other instances of artist names from the song, and is returning the song as one string (NOT a list of strings) with newlines (`\\n`) and punctuation removed and every word in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  t w o   w r o n g s   d o n  t   m a k e   u s   r i g h t   a w a y   t e l l   m e   s o m e t h i n g  s   w r o n g\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_song(song):\n",
    "    # Remove lines that contain artist names in square brackets\n",
    "    song = re.sub(r'\\[.*\\]', '', song)\n",
    "    \n",
    "    # Join the list of strings into one big string\n",
    "    song = ' '.join(song)\n",
    "    \n",
    "    # Remove newline characters\n",
    "    song = song.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove punctuation marks: \",.'?!()\"\n",
    "    song = re.sub(r'[\",.\\'?!()]', '', song)\n",
    "    \n",
    "    # Make every word lowercase\n",
    "    song = song.lower()\n",
    "    \n",
    "    return song\n",
    "\n",
    "# Example song for testing\n",
    "test_song = \"[Kendrick Lamar:]\\nTwo wrongs don't make us right away\\nTell me something's wrong\"\n",
    "clean_test_song = clean_song(test_song)\n",
    "print(clean_test_song)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, we can use `nltk`'s `word_tokenize()` function on the song string to get a fully tokenized version of the song. Test this function on `clean_test_song` to ensure that the function works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'w', 'o', 'w', 'r', 'o', 'n', 'g', 's', 'd', 'o', 'n', 't', 'm', 'a', 'k', 'e', 'u', 's', 'r', 'i', 'g', 'h', 't', 'a', 'w', 'a', 'y', 't', 'e', 'l', 'l', 'm', 'e', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', 's', 'w', 'r', 'o', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Test the word_tokenize() function on clean_test_song\n",
    "tokenized_test_song = word_tokenize(clean_test_song)\n",
    "print(tokenized_test_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we can tokenize our songs, we can move onto vectorization. \n",
    "\n",
    "\n",
    "### Count Vectorization\n",
    "\n",
    "Machine Learning algorithms don't understand strings. However, they do understand math, which means they understand vectors and matrices.  By **_Vectorizing_** the text, we just convert the entire text into a vector, where each element in the vector represents a different word. The vector is the length of the entire vocabulary -- usually, every word that occurs in the English language, or at least every word that appears in our corpus.  Any given sentence can then be represented as a vector where all the vector is 1 (or some other value) for each time that word appears in the sentence. \n",
    "\n",
    "Consider the following example: \n",
    "\n",
    "<center>\"I scream, you scream, we all scream for ice cream.\"</center>\n",
    "\n",
    "| 'aardvark' | 'apple' | [...] | 'I' | 'you' | 'scream' | 'we' | 'all' | 'for' | 'ice' | 'cream' | [...] | 'xylophone' | 'zebra' |\n",
    "|:----------:|:-------:|:-----:|:---:|:-----:|:--------:|:----:|:-----:|:-----:|:-----:|:-------:|:-----:|:-----------:|:-------:|\n",
    "|      0     |    0    |   0   |  1  |   1   |     3    |   1  |   1   |   1   |   1   |    1    |   0   |      0      |    0    |\n",
    "\n",
    "This is called a **_Sparse Representation_**, since the strong majority of the columns will have a value of 0.  Note that elements corresponding to words that do not occur in the sentence have a value of 0, while words that do appear in the sentence have a value of 1 (or 1 for each time it appears in the sentence).\n",
    "\n",
    "Alternatively, we can represent this sentence as a plain old Python dictionary of word frequency counts:\n",
    "\n",
    "```python\n",
    "BoW = {\n",
    "    'I':1,\n",
    "    'you':1,\n",
    "    'scream':3,\n",
    "    'we':1,\n",
    "    'all':1,\n",
    "    'for':1,\n",
    "    'ice':1,\n",
    "    'cream':1\n",
    "}\n",
    "```\n",
    "\n",
    "Both of these are examples of **_Count Vectorization_**. They allow us to represent a sentence as a vector, with each element in the vector corresponding to how many times that word is used.\n",
    "\n",
    "#### Positional Information and Bag of Words\n",
    "\n",
    "Notice that when we vectorize a sentence this way, we lose the order that the words were in.  This is the **_Bag of Words_** approach mentioned earlier.  Note that sentences that contain the same words will create the same vectors, even if they mean different things -- e.g. `'cats are scared of dogs'` and `'dogs are scared of cats'` would both produce the exact same vector, since they contain the same words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, create a function that takes in a tokenized, cleaned song and returns a count vectorized representation of it as a Python dictionary.\n",
    "\n",
    "**_Hint:_**  Consider using a `set()` since you'll need each unique word in the tokenized song! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'n': 4, 'g': 4, 'k': 1, 's': 4, 'l': 2, 'y': 1, 'e': 4, 'w': 4, 'm': 3, 'i': 2, 'r': 3, 't': 5, 'o': 5, 'h': 2, 'd': 1, 'a': 3, 'u': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_vectorize(tokenized_song):\n",
    "    # Initialize a defaultdict to store word counts\n",
    "    count_vector = defaultdict(int)\n",
    "    \n",
    "    # Create a set of unique words in the tokenized song\n",
    "    unique_words = set(tokenized_song)\n",
    "    \n",
    "    # Count the occurrences of each unique word in the tokenized song\n",
    "    for word in unique_words:\n",
    "        count_vector[word] = tokenized_song.count(word)\n",
    "    \n",
    "    return count_vector\n",
    "\n",
    "# Example test\n",
    "test_vectorized = count_vectorize(tokenized_test_song)\n",
    "print(test_vectorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You've just successfully vectorized your first text document! Now, let's look at a more advanced type of vectorization, TF-IDF!\n",
    "\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "TF-IDF stands for **_Term Frequency, Inverse Document Frequency_**.  This is a more advanced form of vectorization that weighs each term in a document by how unique it is to the given document it is contained in, which allows us to summarize the contents of a document using a few key words.  If the word is used often in many other documents, it is not unique, and therefore probably not too useful if we wanted to figure out how this document is unique in relation to other documents. Conversely, if a word is used many times in a document, but rarely in all the other documents we are considering, then it is likely a good indicator for telling us that this word is important to the document in question.  \n",
    "\n",
    "The formula TF-IDF uses to determine the weights of each term in a document is **_Term Frequency_** multiplied by **_Inverse Document Frequency_**. We just calculated our Term Frequency above with Count Vectorization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this, we can easily calculate _Inverse Document Frequency_. Note that this will need ALL of our documents (aka our songs), not just an individual document - so we'll put off testing this function for now.\n",
    "\n",
    "In the cell below, complete a function that takes in a list of tokenized songs, with each item in the list being a clean, tokenized version of the song. The function should return a dictionary containing the inverse document frequency values for each word.  \n",
    "\n",
    "The formula for Inverse Document Frequency is:  \n",
    "<br>  \n",
    "<br>\n",
    "$$\\large \\text{IDF}(t) =  log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with } t \\text{ in it}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def inverse_document_frequency(list_of_token_songs):\n",
    "    # Initialize a dictionary to store IDF values for each word\n",
    "    idf_dict = {}\n",
    "    \n",
    "    # Total number of documents\n",
    "    total_documents = len(list_of_token_songs)\n",
    "    \n",
    "    # Create a set to keep track of which documents contain each word\n",
    "    word_documents = {}\n",
    "    \n",
    "    # Iterate through each document\n",
    "    for tokenized_song in list_of_token_songs:\n",
    "        # Create a set of unique words in the document\n",
    "        unique_words = set(tokenized_song)\n",
    "        \n",
    "        # Update the word_documents set for each unique word\n",
    "        for word in unique_words:\n",
    "            if word in word_documents:\n",
    "                word_documents[word].add(tokenized_song)\n",
    "            else:\n",
    "                word_documents[word] = {tokenized_song}\n",
    "    \n",
    "    # Calculate IDF for each word\n",
    "    for word, documents_containing_word in word_documents.items():\n",
    "        idf = math.log10(total_documents / len(documents_containing_word))\n",
    "        idf_dict[word] = idf\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "# Example usage (you will need a list of tokenized songs)\n",
    "# idf_values = inverse_document_frequency(list_of_token_songs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing TF-IDF\n",
    "\n",
    "Now that we can compute both Term Frequency and Inverse Document Frequency, computing an overall TF-IDF value is simple! All we need to do is multiply the two values.  \n",
    "\n",
    "In the cell below, complete the `tf_idf()` function.  This function should take in a list of tokenized songs, just as the `inverse_document_frequency()` function did.  This function returns a new list of dictionaries, with each dictionary containing the tf-idf vectorized representation of a corresponding song document. You'll need to calculate the term frequency for each song using the `count_vectorize()` function we defined above.\n",
    "\n",
    "**_NOTE:_** Each document should contain the full vocabulary of the entire combined corpus! So, even if a song doesn't have the word \"kikiritikiki\" (a vocalization in our test song), it should have a dictionary entry with that word as the key and `0` as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def tf_idf(list_of_token_songs):\n",
    "    # Calculate IDF values for each word in the corpus\n",
    "    idf_dict = inverse_document_frequency(list_of_token_songs)\n",
    "    \n",
    "    # Initialize a list to store TF-IDF representations of songs\n",
    "    tf_idf_list = []\n",
    "    \n",
    "    # Total number of documents\n",
    "    total_documents = len(list_of_token_songs)\n",
    "    \n",
    "    # Iterate through each song\n",
    "    for tokenized_song in list_of_token_songs:\n",
    "        # Calculate the TF values for the current song\n",
    "        tf_vector = count_vectorize(tokenized_song)\n",
    "        \n",
    "        # Initialize a dictionary for the TF-IDF representation of the current song\n",
    "        tf_idf_dict = defaultdict(float)  # Default value is 0.0\n",
    "        \n",
    "        # Calculate TF-IDF values for each word in the song\n",
    "        for word, tf_value in tf_vector.items():\n",
    "            # Calculate TF-IDF: TF * IDF\n",
    "            tf_idf_value = tf_value * idf_dict.get(word, 0.0)  # Use IDF value or default to 0.0 if word not found\n",
    "            tf_idf_dict[word] = tf_idf_value\n",
    "        \n",
    "        # Append the TF-IDF representation of the current song to the list\n",
    "        tf_idf_list.append(dict(tf_idf_dict))\n",
    "    \n",
    "    return tf_idf_list\n",
    "\n",
    "# Example usage (you will need a list of tokenized songs)\n",
    "# tf_idf_representations = tf_idf(list_of_token_songs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing All Documents\n",
    "\n",
    "Now that we've created all the necessary helper functions, we can load in all of our documents and run each through the vectorization pipeline we've just created.\n",
    "\n",
    "In the cell below, complete the `main()` function.  This function should take in a list of file names (provided for you in the `filenames` list we created at the start), and then:\n",
    "\n",
    "- Read in each document into a list of raw songs (where each song is a list of strings)\n",
    "- Tokenize each document into a list of cleaned and tokenized songs\n",
    "- Return a list of dictionaries vectorized using `tf-idf`, where each dictionary is a vectorized representation of each song "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory path where the song files are located (assuming they are in a \"data\" subdirectory)\n",
    "directory_path = \"data\"\n",
    "\n",
    "# List comprehension to get all the file names in the directory\n",
    "filenames = [os.path.join(directory_path, filename) for filename in os.listdir(directory_path) if filename.endswith(\".txt\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filenames):\n",
    "    # Initialize a list to store raw songs (each song as a list of strings)\n",
    "    raw_songs_list = []\n",
    "\n",
    "    # Read in each document into a list of raw songs\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            raw_song = file.readlines()\n",
    "            raw_songs_list.append(raw_song)\n",
    "\n",
    "    # Tokenize each document into a list of cleaned and tokenized songs\n",
    "    tokenized_songs_list = [clean_song(' '.join(song)) for song in raw_songs_list]\n",
    "\n",
    "    # Return a list of dictionaries vectorized using tf-idf\n",
    "    tf_idf_representations = tf_idf(tokenized_songs_list)\n",
    "    \n",
    "    return tf_idf_representations\n",
    "\n",
    "# Example usage\n",
    "tf_idf_all_docs = main(filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level Up: Visualizing our Vectorizations (Optional)\n",
    "\n",
    "Now that we have a tf-idf representation of each document, we can move on to the fun part -- visualizing everything!\n",
    "\n",
    "In the cell below, examine our dictionaries to figure out how many dimensions our dataset has. \n",
    "\n",
    "**_HINT_**: Remember that every word is its own dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dimensions: 30\n"
     ]
    }
   ],
   "source": [
    "# Choose one of the TF-IDF representations (e.g., the first song's representation)\n",
    "sample_tf_idf = tf_idf_all_docs[0]\n",
    "\n",
    "# Get the number of dimensions by counting the unique words (keys) in the dictionary\n",
    "num_dims = len(sample_tf_idf)\n",
    "\n",
    "print(f\"Number of Dimensions: {num_dims}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many dimensions for us to visualize! In order to make it understandable to human eyes, we'll need to reduce it to 2 or 3 dimensions.  \n",
    "\n",
    "To do this, we'll use a technique called **_t-SNE_** (short for _t-Stochastic Neighbors Embedding_).  This is too complex for us to code ourselves, so we'll make use of scikit-learn's implementation of it.  \n",
    "\n",
    "First, we need to pull the words out of the dictionaries stored in `tf_idf_all_docs` so that only the values remain, and store them in lists instead of dictionaries.  This is because the t-SNE only works with array-like objects, not dictionaries.  \n",
    "\n",
    "In the cell below, create a list of lists that contains a list representation of the values of each of the dictionaries stored in `tf_idf_all_docs`.  The same structure should remain -- e.g. the first list should contain only the values that were in the first dictionary in `tf_idf_all_docs`, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5440680443502757, 0.8450980400142568, 0.0, 31.73326307361406, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Extract the values from each dictionary and store them in a list of lists\n",
    "tf_idf_vals_list = [list(song_dict.values()) for song_dict in tf_idf_all_docs]\n",
    "\n",
    "# Example: Print the first 10 values of the first song's representation\n",
    "print(tf_idf_vals_list[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have only the values, we can use the `TSNE()` class from `sklearn` to transform our data appropriately. In the cell below, instantiate `TSNE()` with the following arguments:\n",
    "- `n_components=3` (so we can compare 2 vs 3 components when graphing)\n",
    "- `perplexity=19` (the highest number of neighbors explored given the size of our dataset)\n",
    "- `learning_rate=200` (a higher learning rate than using 'auto', to avoid getting stuck in a local minimum)\n",
    "- `init='random'` (so SKLearn will randomize the initialization)\n",
    "- `random_state=13` (so that random initialization won't be TOO random)\n",
    "\n",
    "Then, use the created object's `.fit_transform()` method to transform the data stored in `tf_idf_vals_list` into 3-dimensional data.  Then, inspect the newly transformed data to confirm that it has the correct dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def tf_idf(list_of_token_songs):\n",
    "    # Create a list to store the TF-IDF representations of songs\n",
    "    tf_idf_representations = []\n",
    "\n",
    "    # Calculate the total number of songs\n",
    "    total_songs = len(list_of_token_songs)\n",
    "\n",
    "    # Create a list to store the document frequencies of each word\n",
    "    doc_frequencies = Counter()\n",
    "\n",
    "    # Calculate the document frequencies\n",
    "    for song in list_of_token_songs:\n",
    "        unique_words = set(song)\n",
    "        doc_frequencies.update(unique_words)\n",
    "\n",
    "    # Calculate the TF-IDF representations for each song\n",
    "    for song in list_of_token_songs:\n",
    "        tf_idf_representation = {}\n",
    "        word_counts = Counter(song)\n",
    "        total_words = len(song)\n",
    "\n",
    "        for word, count in word_counts.items():\n",
    "            # Calculate TF (Term Frequency)\n",
    "            tf = count / total_words\n",
    "\n",
    "            # Calculate IDF (Inverse Document Frequency)\n",
    "            idf = math.log(total_songs / (doc_frequencies[word] + 1))\n",
    "\n",
    "            # Calculate TF-IDF\n",
    "            tf_idf_value = tf * idf\n",
    "\n",
    "            # Store the TF-IDF value in the representation\n",
    "            tf_idf_representation[word] = tf_idf_value\n",
    "\n",
    "        tf_idf_representations.append(tf_idf_representation)\n",
    "\n",
    "    return tf_idf_representations\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to check out how the visualization looks in 2d.  Repeat the process above, but this time, instantiate `TSNE()` with 2 components instead of 3.  Again, use `.fit_transform()` to transform the data and store it in the variable below, and then inspect it to confirm the transformed data has only 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.0460346e+01,  3.4182117e+00],\n",
       "       [ 9.9807215e+00,  3.6459179e+01],\n",
       "       [ 2.0305151e+01,  1.5964236e+01],\n",
       "       [ 1.1960804e+01,  4.4829906e+01],\n",
       "       [-5.3206627e+01, -7.1542439e+00],\n",
       "       [ 1.3412737e+01,  1.0734321e+01],\n",
       "       [-3.9367794e+01,  2.1730465e+01],\n",
       "       [-2.6485462e+01, -2.4167418e+00],\n",
       "       [-1.2100647e+01, -4.1090939e+01],\n",
       "       [ 3.4264698e+01,  2.0745800e+01],\n",
       "       [ 1.4585128e+01,  2.4478769e+00],\n",
       "       [-3.5819824e+01,  2.9560381e+01],\n",
       "       [ 4.8666679e+01, -3.0255866e+00],\n",
       "       [-5.6029296e+00,  1.3332337e+01],\n",
       "       [-1.9111547e+01,  4.9189709e+01],\n",
       "       [ 6.4738251e+01,  2.4315940e-01],\n",
       "       [ 2.3038130e+01, -1.9711161e+01],\n",
       "       [ 5.1691032e+01,  2.4099829e+01],\n",
       "       [-3.6239495e+00, -3.9641006e+01],\n",
       "       [-4.7754330e+01, -9.9539781e-01],\n",
       "       [-4.1661888e+01, -3.1210737e+01],\n",
       "       [-2.7292929e+01, -1.0934088e+01],\n",
       "       [-2.4788666e-01, -6.2915802e+00],\n",
       "       [ 5.0990135e+01,  5.1859183e+00],\n",
       "       [-1.1276495e+01,  5.1849449e+01],\n",
       "       [ 1.8461855e+01,  3.5346386e+01],\n",
       "       [-1.2905924e+01,  4.3749989e+01],\n",
       "       [ 4.7453861e+01, -1.8444994e+01],\n",
       "       [ 3.9992081e+01, -5.0046835e+00],\n",
       "       [ 1.8155508e+01, -3.0231236e+01],\n",
       "       [ 2.0210121e+01, -5.3690609e+01],\n",
       "       [ 2.6569519e+01, -5.2028469e+01],\n",
       "       [-3.2234099e+00,  2.1168497e+01],\n",
       "       [ 1.5026016e+01, -2.2379618e+01],\n",
       "       [-5.4837637e+00, -3.1268013e+01],\n",
       "       [ 2.2776302e+01, -1.3867801e-01],\n",
       "       [-1.1024405e+01,  1.9276054e+01],\n",
       "       [ 2.6313334e+01, -2.7560150e+01],\n",
       "       [ 4.4423164e+01,  1.0791097e+01],\n",
       "       [ 8.0063008e-04, -1.2665015e+01],\n",
       "       [-3.7393551e+01, -3.6082638e+01],\n",
       "       [-2.8223459e+01,  2.5593685e+01],\n",
       "       [-1.8875048e+01, -1.1192273e+01],\n",
       "       [-1.3902643e+01, -3.2675808e+01],\n",
       "       [ 2.0526451e+01,  4.3636322e+01],\n",
       "       [-5.5769394e+01,  7.7181679e-01],\n",
       "       [ 2.3709116e+01,  8.2593479e+00],\n",
       "       [-3.1724483e+01,  1.7813387e+01],\n",
       "       [-1.8090290e+01, -2.7005742e+00]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create a list of dictionaries containing TF-IDF values for each song\n",
    "tf_idf_vals_list = tf_idf(tokenized_test_song)\n",
    "\n",
    "# Convert the list of dictionaries to a NumPy 2D array\n",
    "all_words = set(word for tfidf_values in tf_idf_vals_list for word in tfidf_values)\n",
    "data_matrix = np.zeros((len(tf_idf_vals_list), len(all_words)))\n",
    "\n",
    "# Fill in the data matrix with TF-IDF values\n",
    "for i, song_dict in enumerate(tf_idf_vals_list):\n",
    "    for word, tfidf_value in song_dict.items():\n",
    "        j = list(all_words).index(word)  # Find the column index for the word\n",
    "        data_matrix[i, j] = tfidf_value\n",
    "\n",
    "# Instantiate TSNE with 2 components for 2D visualization\n",
    "t_sne_object_2d = TSNE(n_components=2, perplexity=19, learning_rate=200, init='random', random_state=13)\n",
    "\n",
    "# Transform the data into 2-dimensional space\n",
    "transformed_data_2d = t_sne_object_2d.fit_transform(data_matrix)\n",
    "\n",
    "# Inspect the transformed data (2D)\n",
    "transformed_data_2d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize everything!  Run the cell below to view both 3D and 2D visualizations of the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_data_3d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m kendrick_3d \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed_data_3d\u001b[49m[:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m      2\u001b[0m k3_x \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m kendrick_3d]\n\u001b[0;32m      3\u001b[0m k3_y \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m kendrick_3d]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transformed_data_3d' is not defined"
     ]
    }
   ],
   "source": [
    "kendrick_3d = transformed_data_3d[:10]\n",
    "k3_x = [i[0] for i in kendrick_3d]\n",
    "k3_y = [i[1] for i in kendrick_3d]\n",
    "k3_z = [i[2] for i in kendrick_3d]\n",
    "\n",
    "garth_3d = transformed_data_3d[10:]\n",
    "g3_x = [i[0] for i in garth_3d]\n",
    "g3_y = [i[1] for i in garth_3d]\n",
    "g3_z = [i[2] for i in garth_3d]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(k3_x, k3_y, k3_z, c='b', s=60, label='Kendrick')\n",
    "ax.scatter(g3_x, g3_y, g3_z, c='red', s=60, label='Garth')\n",
    "ax.view_init(40,10)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "kendrick_2d = transformed_data_2d[:10]\n",
    "k2_x = [i[0] for i in kendrick_2d]\n",
    "k2_y = [i[1] for i in kendrick_2d]\n",
    "\n",
    "garth_2d = transformed_data_2d[10:]\n",
    "g2_x = [i[0] for i in garth_2d]\n",
    "g2_y = [i[1] for i in garth_2d]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(k2_x, k2_y, c='b', label='Kendrick')\n",
    "ax.scatter(g2_x, g2_y, c='red', label='Garth')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kendrick_2d = transformed_data_2d[:10]\n",
    "k2_x = [i[0] for i in kendrick_2d]\n",
    "k2_y = [i[1] for i in kendrick_2d]\n",
    "\n",
    "garth_2d = transformed_data_2d[10:]\n",
    "g2_x = [i[0] for i in garth_2d]\n",
    "g2_y = [i[1] for i in garth_2d]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(k2_x, k2_y, c='b', label='Kendrick')\n",
    "ax.scatter(g2_x, g2_y, c='red', label='Garth')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Take a crack at interpreting these graphs by answering the following questions below:\n",
    "\n",
    "What does each graph mean? Do you find one graph more informative than the other? Do you think that this method shows us discernable differences between Kendrick Lamar songs and Garth Brooks songs?  Use the graphs and your understanding of TF-IDF to support your answer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "WRITE YOUR ANSWER HERE\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to: \n",
    "* Tokenize a corpus of words and identify the different choices to be made while parsing them \n",
    "* Use a count vectorization strategy to create a bag of words\n",
    "* Use TF-IDF vectorization with multiple documents to identify words that are important/unique to certain documents\n",
    "* Visualize and compare vectorized text documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
